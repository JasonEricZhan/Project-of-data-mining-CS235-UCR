{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "train = pd.read_csv('/Users/tinatsai/Documents/GitHub/kkbox-music-recommendation-challenge/input/train.csv')[:500]\n",
    "\n",
    "songs = pd.read_csv('/Users/tinatsai/Documents/GitHub/kkbox-music-recommendation-challenge/input/songs.csv')\n",
    "#print(songs.head())\n",
    "members = pd.read_csv('/Users/tinatsai/Documents/GitHub/kkbox-music-recommendation-challenge/input/members.csv')\n",
    "#print(members.head(20))\n",
    "train=train.merge(members, left_on='msno', right_on='msno', how='inner')\n",
    "train['teller']=1.0\n",
    "print(train.shape,train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipped songs get rating 1, listened songs get rating 5\n",
    "train['rating']=train['target']*5-1\n",
    "train.fillna(value=0,axis=1,inplace=True)\n",
    "\n",
    "topusers=train.groupby(by=['msno'])['rating'].sum()\n",
    "topsongs=train.groupby(by=['song_id'])['rating'].sum()\n",
    "topsongs=topsongs.sort_values(0,ascending=False)   #[:20000]\n",
    "print(topsongs)\n",
    "#3.5M songs, we limit to top 30K songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2vect(df,uid,pid,rate):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from sklearn.preprocessing import normalize\n",
    "    toppid=train.groupby(by=[pid])[rate].sum()\n",
    "    toppid=toppid.sort_values(0,ascending=False)   #[:20000]\n",
    "    print(toppid)    \n",
    "    #sparse matrix with product in rows and users in columns\n",
    "    df=df[df[pid].isin(toppid.index)]\n",
    "    user_u = list(df[uid].unique())\n",
    "    song_u = list(toppid.index)\n",
    "    col = df[uid].astype('category', category=user_u).cat.codes\n",
    "    row = df[pid].astype('category', category=song_u).cat.codes\n",
    "    songrating = csr_matrix((df[df[pid].isin(song_u)][rate].tolist(), (row,col)), shape=(len(song_u),len(user_u)))\n",
    "    \n",
    "    #normalize\n",
    "    songrating_n = normalize(songrating, norm='l1', axis=0)\n",
    "    return songrating_n,toppid.index\n",
    "\n",
    "ratings,songU = trans2vect(train,'msno','song_id','teller')\n",
    "ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender,genderU =trans2vect(train,'msno','gender','teller')\n",
    "gender\n",
    "from scipy.sparse import vstack\n",
    "ratings=vstack((ratings,gender))\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bds,bdU =trans2vect(train,'msno','bd','teller')\n",
    "bds\n",
    "from scipy.sparse import vstack\n",
    "ratings=vstack((ratings,bds))\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered,registerU =trans2vect(train,'msno','registered_via','teller')\n",
    "registered\n",
    "from scipy.sparse import vstack\n",
    "ratings=vstack((ratings,registered))\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds       \n",
    "#similarities = cosine_similarity(ratings)  #goes south with >15k songs\n",
    "#print(similarities.shape)\n",
    "#similarities\n",
    "\n",
    "SongsU,Eigen,UsersU=svds(ratings, k=100)\n",
    "\n",
    "#print(Eigen)\n",
    "#print(SongsU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userU = list(train['msno'].unique())\n",
    "index1=songU.append(genderU)\n",
    "index2=index1.append(bdU)\n",
    "index3=index2.append(registerU)\n",
    "similarities = pd.DataFrame( cosine_similarity(SongsU,UsersU.T) , index=index3,columns=userU)\n",
    "\n",
    "#find similar users\n",
    "similarities.sort_values('T86YHdD4C9JSc274b1IlMkLuNdz4BQRB50fWWE7hx9g=',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top songs to recommend\n",
    "similarities.sort_values('FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=',axis=0)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the third train record\n",
    "similarities.loc[train.iloc[3]['song_id'],train.iloc[3]['msno']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['msno', 'song_id', 'source_system_tab', 'source_screen_name',\n",
      "       'source_type', 'target', 'city', 'bd', 'gender', 'registered_via',\n",
      "       'registration_init_time', 'expiration_date', 'teller', 'rating', 'sim'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train['sim']=0.0\n",
    "print(train.columns)\n",
    "\n",
    "for xi in range(0,len(train)):\n",
    "    train.iat[xi,14]=similarities.loc[train.iloc[xi]['song_id'],train.iloc[xi]['msno']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0   -0.004947\n",
      "1    0.010104\n",
      "Name: sim, dtype: float64\n",
      "target\n",
      "0    0.066114\n",
      "1    0.132426\n",
      "Name: sim, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print( train.groupby(by=['target'])['sim'].mean() )\n",
    "print( train.groupby(by=['target'])['sim'].std() )\n",
    "#split between 0 and 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 649.29it/s]\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno: 126, 193\n",
      "song_id: 452, 457\n",
      "source_system_tab: 7, 6\n",
      "source_screen_name: 13, 14\n",
      "source_type: 10, 10\n",
      "gender: 3, 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [50, 450]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-ed8fad609689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mx_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [50, 450]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 21 20:41:10 2017\n",
    "@author: eric\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    g=1/(np.exp(-z)+1)\n",
    "    return g\n",
    "\n",
    "\n",
    "def cross_entropy(x,w,y):\n",
    "    loss=0\n",
    "    row=np.shape(y)[0]\n",
    "    \n",
    "    for i in range(0,row):\n",
    "      if y[i]==1:\n",
    "          loss=loss-np.log(sigmoid(np.dot(x[i],w)))\n",
    "      else:\n",
    "          loss=loss-np.log(sigmoid(1-np.dot(x[i],w)))\n",
    "    \n",
    "    loss=loss/row\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def gradient(x,w,y,l2reg,size,count):\n",
    "    gradient_error=0\n",
    "    row=np.shape(x)[0]\n",
    "    \n",
    "    if(size==row): # batch\n",
    "        for i in range(0,row):\n",
    "             pred=sigmoid(np.dot(x[i],w))\n",
    "             gradient_error=gradient_error+(pred-y[i])*x[i]+(l2reg/row)*w\n",
    "        gradient_error=gradient_error/row\n",
    "    \n",
    "    else:  # minibatch\n",
    "        range_array=np.arange(0,row,size)\n",
    "        for i in range(range_array[count],size+range_array[count]):\n",
    "            if i >= row:\n",
    "                size=(i-range_array[count])+1\n",
    "                break\n",
    "            else:\n",
    "                pred=sigmoid(np.dot(x[i],w))\n",
    "                gradient_error=gradient_error+(pred-y[i])*x[i]+(l2reg/row)*w\n",
    "        gradient_error=gradient_error/size\n",
    "    \n",
    "    return gradient_error\n",
    " \n",
    "\n",
    "class logistic_regression(object):\n",
    "      def __init__(self,eta,l2reg=0,maxiter=1000,size=0):\n",
    "          self.eta=eta\n",
    "          self.iter=maxiter\n",
    "          self.l2reg=l2reg #with l2 regularizer ,lambda coefficient in lagrange  mutiplier\n",
    "          self.batch_size=size\n",
    "          \n",
    "      def fit(self,X,Y):\n",
    "          ones = np.ones((np.shape(X)[0], 1))\n",
    "          X= np.concatenate((ones, X), axis=1)\n",
    "          row=np.shape(X)[0]\n",
    "          #w = np.random.randn(np.shape(X)[1]+ 1)\n",
    "          w=np.zeros(np.shape(X)[1])\n",
    "          iter_last=self.iter\n",
    "          costs_record = []\n",
    "          if(self.batch_size==0):\n",
    "            self.batch_size=row\n",
    "         \n",
    "          count=0\n",
    "          w=w-self.eta* gradient(X,w,Y,self.l2reg,self.batch_size,count)\n",
    "          for i in range(0,self.iter):\n",
    "              count+=1\n",
    "              if count*self.batch_size >= row:\n",
    "                 count=0\n",
    "              costs_record.append(cross_entropy(X,w,Y))\n",
    "              #loss function is separate to regularizer but include in gradient(augmenting function)\n",
    "              if np.all(gradient(X,w,Y,self.l2reg,self.batch_size,count) ==0):\n",
    "                 iter_last=i\n",
    "                 break\n",
    "              w=w-self.eta*gradient(X,w,Y,self.l2reg,self.batch_size,count)\n",
    "          \n",
    "          plt.clf()\n",
    "          #plt.plot(range(0,iter_last), costs_record)\n",
    "          plt.show()\n",
    "          self.w=w\n",
    "          return self\n",
    "          \n",
    "      def predict(self,test_X):\n",
    "          ones = np.ones((np.shape(test_X)[0], 1))\n",
    "          test_X= np.concatenate((ones, test_X), axis=1)\n",
    "          row=np.shape(test_X)[0]\n",
    "          predict_y=[]\n",
    "          #probability threshold :0.5\n",
    "          for i in range(0,row):\n",
    "              if sigmoid(np.dot(test_X[i],self.w))>0.5:\n",
    "                 answer=1\n",
    "              else:\n",
    "                 answer=0\n",
    "              predict_y.append(answer)\n",
    "          predict_y=np.array(predict_y)\n",
    "          return predict_y\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "test = pd.read_csv('/Users/tinatsai/Documents/GitHub/kkbox-music-recommendation-challenge/input/test.csv')[:500]\n",
    "test=test.merge(members, left_on='msno', right_on='msno', how='inner')\n",
    "\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "        print(col + ': ' + str(len(train_vals)) + ', ' + str(len(test_vals)))\n",
    "\n",
    "X = np.array(train.drop(['target','rating'], axis=1))\n",
    "y = train['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "ids = test['id'].values\n",
    "\n",
    "del train, test\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state = 12)\n",
    "\n",
    "model=logistic_regression(0.01,l2reg=5,maxiter=100,size=10)\n",
    "model.fit(X_train,y_train)\n",
    "x_predict = model.predict(X_valid)\n",
    "\n",
    "accuracy_score(y_valid, x_predict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
